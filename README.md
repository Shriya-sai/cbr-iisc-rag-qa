ğŸ§  CBR IISc Annual Report QA â€” Local RAG System
A fully local Retrieval Augmented Generation (RAG) question-answering system that answers questions about the last 3 years of annual reports from the Centre for Brain Research (CBR), IISc.
This project was built as an end-to-end Machine Learning + LLM engineering project for technical interviews.

âœ¨ Features

ğŸ  100% local â€” no paid APIs
ğŸ”“ Uses open-source LLM (Mistral via Ollama)
ğŸ“„ Answers questions from real PDF reports
ğŸ“Œ Provides citations with year + page
âœ… Includes evaluation pipeline
ğŸ’¬ Comes with a Streamlit chat UI


ğŸ¥ Demo
Ask questions like:

What is the GenomeIndia project?
What studies does CBR conduct on aging?
Who funds the GenomeIndia project?

The system retrieves relevant report sections and generates grounded answers with citations.

ğŸ—ï¸ System Architecture
User Question
     â†“
Retriever (FAISS Vector DB)
     â†“
Top-K Relevant Chunks
     â†“
Prompt Construction
     â†“
Local LLM (Mistral via Ollama)
     â†“
Answer + Citations
Pipeline Overview

Data Ingestion

Automatically downloads latest CBR annual reports (PDFs)
Extracts text page-by-page


Pre-processing

Cleans and normalizes text
Splits into semantic chunks


Embedding + Vector Store

SentenceTransformer embeddings
FAISS local vector database


RAG Pipeline

Retrieve relevant chunks
Build grounded prompt
Generate answer with citations


Evaluation

Retrieval quality testing
Answer faithfulness testing


User Interface

Streamlit chat app




ğŸ› ï¸ Tech Stack
ComponentToolLanguagePythonLLMMistral (Ollama)FrameworkLangChainEmbeddingsSentenceTransformersVector DBFAISS (local)UIStreamlitPDF Parsingpdfplumber

âš™ï¸ Setup Instructions
1ï¸âƒ£ Clone the repository
bashgit clone https://github.com/<your-username>/cbr-iisc-rag-qa.git
cd cbr-iisc-rag-qa
2ï¸âƒ£ Create virtual environment
bashpython -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
3ï¸âƒ£ Install dependencies
bashpip install -r requirements.txt
4ï¸âƒ£ Install Ollama
Download from: https://ollama.com
bashollama pull mistral
ollama serve

ğŸ“¥ Build the Knowledge Base
bashpython -m src.ingestion.download_reports
python -m src.ingestion.pdf_parser
python -m src.ingestion.text_cleaning
python -m src.chunking.chunk_documents
python -m src.embeddings.build_vector_store

ğŸ’¬ Run the Assistant (Terminal)
bashpython -m src.rag.rag_pipeline

ğŸ–¥ï¸ Run the Web App
bashstreamlit run app/streamlit_app.py
Open in browser: http://localhost:8501

ğŸ“Š Evaluation
Run the evaluation suite:
bashpython -m src.evaluation.evaluate
This tests:

âœ… Retrieval relevance
âœ… Generation faithfulness
âœ… Citation grounding


ğŸ¯ Design Decisions
Why RAG?

Annual reports are long, domain-specific documents
RAG grounds LLM responses in real documents and prevents hallucinations

Why Local LLM?

âœ… No paid APIs
âœ… Fully offline
âœ… Reproducible
âœ… Demonstrates real engineering skills

Why FAISS?

âš¡ Fast local similarity search
ğŸ  No external infrastructure required

Why PDF Page Citations?

Annual reports have inconsistent printed page numbers
Using deterministic PDF page numbers ensures reliable and verifiable citations


ğŸ§ª Evaluation Strategy
We evaluate two aspects:
Retrieval Quality
Check whether top-k retrieved pages are relevant for benchmark questions.
Answer Faithfulness
Verify that generated answers:

âœ… Use retrieved context
âœ… Provide citations
âœ… Avoid hallucinations


ğŸš€ Future Improvements

 Hybrid search (BM25 + embeddings)
 Reranking models
 Conversation memory
 Cloud deployment
