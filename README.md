# ğŸ§  CBR IISc Annual Report QA â€” Local RAG System

![Python](https://img.shields.io/badge/Python-3.9+-blue)
![RAG](https://img.shields.io/badge/LLM-RAG-green)
![Ollama](https://img.shields.io/badge/LLM-Mistral-orange)
![License](https://img.shields.io/badge/License-MIT-lightgrey)

A fully local **Retrieval Augmented Generation (RAG)** question-answering system that answers questions about the last 3 years of annual reports from the **Centre for Brain Research (CBR), IISc**.

This project was built as an end-to-end Machine Learning + LLM engineering project for technical interviews.

---

## âœ¨ Features

- ğŸ  100% **local** â€” no paid APIs
- ğŸ”“ Uses **open-source LLM (Mistral via Ollama)**
- ğŸ“„ Answers questions from real PDF reports
- ğŸ“Œ Provides **citations with year + page**
- âœ… Includes **evaluation pipeline**
- ğŸ’¬ Comes with a **Streamlit chat UI**

---

## ğŸ¥ Demo

Ask questions like:
- *What is the GenomeIndia project?*
- *What studies does CBR conduct on aging?*
- *Who funds the GenomeIndia project?*

The system retrieves relevant report sections and generates grounded answers with citations.

---

## ğŸ—ï¸ System Architecture
```mermaid
flowchart LR
    A[User Question] --> B[Retriever<br/>FAISS Vector DB]
    B --> C[Relevant Document Chunks]
    C --> D[Prompt Builder]
    D --> E[Local LLM<br/>Mistral via Ollama]
    E --> F[Answer + Citations]
```

### Pipeline Overview

1. **Data Ingestion**
   - Automatically downloads latest CBR annual reports (PDFs)
   - Extracts text page-by-page

2. **Pre-processing**
   - Cleans and normalizes text
   - Splits into semantic chunks

3. **Embedding + Vector Store**
   - SentenceTransformer embeddings
   - FAISS local vector database

4. **RAG Pipeline**
   - Retrieve relevant chunks
   - Build grounded prompt
   - Generate answer with citations

5. **Evaluation**
   - Retrieval quality testing
   - Answer faithfulness testing

6. **User Interface**
   - Streamlit chat app

---

## ğŸ“‚ Project Structure
```mermaid
graph TD
    A[cbr-iisc-rag-qa/] --> B[app/]
    A --> C[src/]
    A --> D[tests/]
    A --> E[requirements.txt]
    A --> F[README.md]
    
    B --> B1[streamlit_app.py]
    
    C --> C1[ingestion/]
    C --> C2[chunking/]
    C --> C3[embeddings/]
    C --> C4[rag/]
    C --> C5[evaluation/]
    C --> C6[utils/]
    
    C1 --> C1A[download_reports.py]
    C1 --> C1B[pdf_parser.py]
    C1 --> C1C[text_cleaning.py]
    
    C2 --> C2A[chunk_documents.py]
    
    C3 --> C3A[build_vector_store.py]
    
    C4 --> C4A[rag_pipeline.py]
    
    C5 --> C5A[evaluate.py]
    
    C6 --> C6A[config.py]
    C6 --> C6B[report_links.py]
```

---

## ğŸ› ï¸ Tech Stack

| Component | Tool |
|-----------|------|
| Language | Python |
| LLM | Mistral (Ollama) |
| Framework | LangChain |
| Embeddings | SentenceTransformers |
| Vector DB | FAISS (local) |
| UI | Streamlit |
| PDF Parsing | pdfplumber |

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/<your-username>/cbr-iisc-rag-qa.git
cd cbr-iisc-rag-qa
```

### 2ï¸âƒ£ Create virtual environment
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Install Ollama

Download from: https://ollama.com
```bash
ollama pull mistral
ollama serve
```

---

## ğŸ“¥ Build the Knowledge Base
```bash
python -m src.ingestion.download_reports
python -m src.ingestion.pdf_parser
python -m src.ingestion.text_cleaning
python -m src.chunking.chunk_documents
python -m src.embeddings.build_vector_store
```

---

## ğŸ’¬ Run the Assistant (Terminal)
```bash
python -m src.rag.rag_pipeline
```

---

## ğŸ–¥ï¸ Run the Web App
```bash
streamlit run app/streamlit_app.py
```

Open in browser: http://localhost:8501

---

## ğŸ“Š Evaluation

Run the evaluation suite:
```bash
python -m src.evaluation.evaluate
```

This tests:
- âœ… Retrieval relevance
- âœ… Generation faithfulness
- âœ… Citation grounding

---

## ğŸ¯ Design Decisions

### Why RAG?
- Annual reports are long, domain-specific documents
- RAG grounds LLM responses in real documents and prevents hallucinations

### Why Local LLM?
- âœ… No paid APIs
- âœ… Fully offline
- âœ… Reproducible
- âœ… Demonstrates real engineering skills

### Why FAISS?
- âš¡ Fast local similarity search
- ğŸ  No external infrastructure required

### Why PDF Page Citations?
- Annual reports have inconsistent printed page numbers
- Using deterministic PDF page numbers ensures reliable and verifiable citations

---

## ğŸ§ª Evaluation Strategy

We evaluate two aspects:

### Retrieval Quality
Check whether top-k retrieved pages are relevant for benchmark questions.

### Answer Faithfulness
Verify that generated answers:
- âœ… Use retrieved context
- âœ… Provide citations
- âœ… Avoid hallucinations

---

## ğŸš€ Future Improvements

- [ ] Hybrid search (BM25 + embeddings)
- [ ] Reranking models
- [ ] Conversation memory
- [ ] Cloud deployment
